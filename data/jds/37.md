About the job
An AI-powered maritime intelligence platform requires a freelance full stack developer to establish a production-grade Pinecone vector database and supporting backend for knowledge retrieval. The role focuses on designing and implementing a robust retrieval stack (RAG) that ingests maritime data, stores high-quality embeddings with rich metadata, and exposes secure, scalable APIs for downstream applications. The solution should integrate with PostgreSQL as a system of record, support efficient updates, and deliver low-latency semantic and hybrid search. The ideal candidate combines data engineering pragmatism with product-minded execution, ensuring documentation, observability, and maintainability are first-class. Work is fully remote, with scope and milestones defined collaboratively.


Deliverables

Design and provision a Pinecone index (pods/collections) with appropriate dimensionality, metric, and replication/sharding strategy for target latency and scale.
Metadata schema and namespaces enabling faceted filtering (e.g., vessel type, geography, timestamp, source) and multi-tenant separation where needed.
Embedding pipeline: chunking strategy, text normalization, deduplication, PII handling, and embedding generation (OpenAI/Cohere/Voyage or equivalent), with retries and backoff.
Ingestion & sync service to upsert vectors from PostgreSQL and external maritime sources (e.g., AIS feeds, vessel registries, reports), including change-data-capture or periodic batch jobs.
Minimal reference app (e.g., Next.js/Node or Python/FastAPI) demonstrating query, filter, and retrieval flows, including API keys and role-based access.


Requirements

Demonstrable experience building vector-based retrieval systems using Pinecone (index design, metadata filtering, upserts, query optimization).
Strong proficiency with PostgreSQL (schema design, indexing, ETL/ELT) and integrating it as the source of truth for embedding pipelines.
Familiarity with LLM/RAG patterns: embedding model selection, chunking heuristics, reranking, caching, and evaluation of retrieval quality.
Cloud fluency (AWS/GCP/Azure), containerization (Docker), CI/CD, and infrastructure-as-code.
Data engineering skills: streaming/batch ingestion, file formats (JSONL/Parquet), and job orchestration (e.g., Airflow, Temporal, or serverless schedulers).
Security best practices for secrets, token management, and access control; comfort with monitoring/observability tooling.
Excellent communication, proactive status updates, and ability to work asynchronously in a remote setup.


About Twine

Twine is a leading freelance marketplace connecting top freelancers, consultants, and contractors with companies needing creative and tech expertise. Trusted by Fortune 500 companies and innovative startups alike, Twine enables companies to scale their teams globally.


Our Mission

Twine's mission is to empower creators and businesses to thrive in an AI-driven, freelance-first world.


European Economic Area